# -*- coding: utf-8 -*-
import os
import sys
import json
import time
import logging
import re
import shutil
from datetime import datetime, timezone, timedelta, date
from pathlib import Path
from collections import defaultdict
from urllib.parse import urlparse

import requests
import yaml
import base64
from markdownify import markdownify as md
import template  # HTML æ¨¡æ¿æ¨¡å—

# --- API è¯·æ±‚é…ç½®å¸¸é‡ ---
POSTS_PER_REQUEST = 40      # æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§å¸–å­æ•°é‡ (Mastodon API é™åˆ¶)
RATE_LIMIT_THRESHOLD = 10   # é€Ÿç‡é™åˆ¶å®‰å…¨é˜ˆå€¼ï¼Œä½äºæ­¤å€¼æ—¶è§¦å‘ç­‰å¾…
DEFAULT_WAIT_TIME = 300     # é»˜è®¤ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå½“æ— æ³•è§£æé€Ÿç‡é™åˆ¶é‡ç½®æ—¶é—´æ—¶ä½¿ç”¨

# --- é…ç½®æ—¥å¿—è®°å½• ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    stream=sys.stdout,
)

# --- è¾…åŠ©å‡½æ•° ---

def load_css_styles():
    """
    ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½ CSS æ ·å¼
    """
    try:
        # å°è¯•ä»æœ¬åœ°æ–‡ä»¶è¯»å–
        css_file = Path("styles/mastodon.css")
        if css_file.exists():
            with open(css_file, "r", encoding="utf-8") as f:
                return f.read()
        else:
            logging.warning("CSS æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
            return get_default_css()
    except Exception as e:
        logging.error(f"è¯»å– CSS æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
        return get_default_css()

def get_default_css():
    """
    è¿”å›é»˜è®¤çš„å†…è” CSSï¼ˆä½œä¸ºåå¤‡ï¼‰
    """
    return """
    /* åŸºç¡€æ ·å¼åå¤‡æ–¹æ¡ˆ */
    body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; }
    .container { max-width: 600px; margin: 0 auto; }
    .status { background: white; padding: 1rem; margin-bottom: 1rem; border-radius: 8px; }
    """

def validate_post_data(post_data):
    """
    éªŒè¯å¸–å­æ•°æ®çš„å®‰å…¨æ€§
    """
    try:
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ["id", "content", "created_at", "account"]
        for field in required_fields:
            if field not in post_data:
                raise ValueError(f"å¸–å­æ•°æ®ç¼ºå°‘å¿…éœ€å­—æ®µï¼š{field}")

        # éªŒè¯ ID æ ¼å¼ï¼ˆåº”è¯¥æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼‰
        try:
            int(post_data["id"])
        except (ValueError, TypeError):
            raise ValueError("å¸–å­ ID æ ¼å¼æ— æ•ˆ")

        # éªŒè¯å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢è¿‡é•¿çš„å†…å®¹
        if len(str(post_data["content"])) > 100000:  # 100KB é™åˆ¶
            raise ValueError("å¸–å­å†…å®¹è¿‡é•¿ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨éšæ‚£")

        # éªŒè¯åˆ›å»ºæ—¶é—´æ ¼å¼
        if not isinstance(post_data["created_at"], str):
            raise ValueError("åˆ›å»ºæ—¶é—´æ ¼å¼æ— æ•ˆ")

        # éªŒè¯è´¦æˆ·ä¿¡æ¯
        if "account" in post_data and post_data["account"]:
            account = post_data["account"]
            if "display_name" in account and len(str(account["display_name"])) > 1000:
                raise ValueError("ç”¨æˆ·æ˜¾ç¤ºåç§°è¿‡é•¿")

        return True

    except Exception as e:
        logging.warning(f"å¸–å­æ•°æ®éªŒè¯å¤±è´¥ï¼š{e}")
        return False

def validate_config(config):
    """
    éªŒè¯é…ç½®çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§
    """
    try:
        # éªŒè¯ Mastodon é…ç½®
        mastodon_config = config["mastodon"]

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["instance_url", "user_id", "access_token"]
        for field in required_fields:
            if not mastodon_config.get(field):
                raise ValueError(f"é…ç½®é”™è¯¯ï¼šç¼ºå°‘å¿…éœ€çš„ Mastodon é…ç½®å­—æ®µï¼š{field}")

        # éªŒè¯ URL æ ¼å¼
        instance_url = mastodon_config["instance_url"]
        if not instance_url.startswith(("http://", "https://")):
            raise ValueError("é…ç½®é”™è¯¯ï¼šinstance_url å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´")

        # éªŒè¯ user_id æ˜¯å¦ä¸ºæ•°å­—
        try:
            int(mastodon_config["user_id"])
        except ValueError:
            raise ValueError("é…ç½®é”™è¯¯ï¼šuser_id å¿…é¡»æ˜¯æ•°å­—")

        # éªŒè¯ access_token é•¿åº¦
        if len(mastodon_config["access_token"]) < 10:
            raise ValueError("é…ç½®é”™è¯¯ï¼šaccess_token é•¿åº¦ä¸è¶³")

        logging.info("âœ” é…ç½®éªŒè¯é€šè¿‡")
        return True

    except KeyError as e:
        raise ValueError(f"é…ç½®é”™è¯¯ï¼šç¼ºå°‘é…ç½®é¡¹ {e}")
    except Exception as e:
        logging.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥ï¼š{e}")
        raise

def get_config():
    """
    æ ¹æ®è¿è¡Œç¯å¢ƒï¼ˆGitHub Actions æˆ–æœ¬åœ°ï¼‰åŠ è½½é…ç½®ã€‚
    """
    # å¦‚æœåœ¨ GitHub Actions ç¯å¢ƒä¸­
    if os.environ.get("GITHUB_ACTIONS") == "true":
        logging.info("âœ” æ£€æµ‹åˆ° GitHub Actions ç¯å¢ƒï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ã€‚")
        return {
            "mastodon": {
                "instance_url": os.environ.get("MASTODON_INSTANCE_URL"),
                "user_id": os.environ.get("MASTODON_USER_ID"),
                "access_token": os.environ.get("MASTODON_ACCESS_TOKEN"),
            },
            "backup": {
                "path": ".", # åœ¨ Actions ä¸­ï¼Œè·¯å¾„å°±æ˜¯ä»“åº“æ ¹ç›®å½•
                "filename": os.environ.get("ARCHIVE_FILENAME") or "archive.md",
                "posts_folder": os.environ.get("POSTS_FOLDER") or "mastodon",
                "media_folder": os.environ.get("MEDIA_FOLDER") or "media",
                "summary_filename": os.environ.get("SUMMARY_FILENAME") or "README.md",
                "html_filename": os.environ.get("HTML_FILENAME") or "index.html",
            },
            "sync": {
                "state_file": "sync_state.json",
                "china_timezone": os.environ.get("CHINA_TIMEZONE", "false").lower() == "true"
            }
        }

    # å¦‚æœæ˜¯æœ¬åœ°è¿è¡Œï¼Œåˆ™ä» config.yaml åŠ è½½
    logging.info("âœ” æœ¬åœ°è¿è¡Œæ¨¡å¼ï¼Œå°è¯•ä» config.yaml æ–‡ä»¶åŠ è½½ã€‚")
    try:
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        logging.info("âœ” é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸã€‚")
        # ç¡®ä¿ backup éƒ¨åˆ†å­˜åœ¨å¹¶æä¾›é»˜è®¤å€¼
        backup_conf = config.setdefault("backup", {})
        backup_conf.setdefault("path", ".")
        backup_conf.setdefault("posts_folder", "mastodon")
        backup_conf.setdefault("filename", "archive.md")
        backup_conf.setdefault("media_folder", "media")
        backup_conf.setdefault("summary_filename", "README.md")

        # ç¡®ä¿ sync éƒ¨åˆ†å­˜åœ¨å¹¶æä¾›é»˜è®¤å€¼
        sync_conf = config.setdefault("sync", {})
        sync_conf.setdefault("china_timezone", False)

        return config
    except FileNotFoundError:
        logging.error("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° config.yaml æ–‡ä»¶ã€‚ç¨‹åºæ— æ³•è¿è¡Œã€‚")
        sys.exit(1)
    except yaml.YAMLError as e:
        logging.error(f"âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼š{e}")
        sys.exit(1)

def get_timezone_aware_datetime(created_at_str, china_timezone=False):
    """
    æ ¹æ®é…ç½®çš„æ—¶åŒºè®¾ç½®è½¬æ¢æ—¶é—´å­—ç¬¦ä¸²

    Args:
        created_at_str: ISO æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²
        china_timezone: æ˜¯å¦ä½¿ç”¨ä¸­å›½æ—¶åŒºï¼ˆGMT+8ï¼‰

    Returns:
        è½¬æ¢åçš„å¸¦æ—¶åŒºçš„ datetime å¯¹è±¡
    """
    dt = datetime.strptime(created_at_str, "%Y-%m-%dT%H:%M:%S.%fZ").replace(tzinfo=timezone.utc)

    if china_timezone:
        # ä½¿ç”¨ä¸­å›½æ—¶åŒº (GMT+8)
        return dt.astimezone(timezone(timedelta(hours=8)))
    else:
        # ä½¿ç”¨ UTC
        return dt

def parse_rate_limit_reset(reset_header):
    """è§£æ Mastodon API çš„ X-RateLimit-Reset æ—¶é—´æˆ³"""
    try:
        # å°è¯•è§£æä¸º Unix æ—¶é—´æˆ³ï¼ˆç§’æ•°ï¼‰
        return int(reset_header)
    except ValueError:
        pass  # ç»§ç»­å°è¯•å…¶ä»–æ ¼å¼

    try:
        # å°è¯•è§£æä¸º ISO æ ¼å¼æ—¶é—´æˆ³
        if 'T' in reset_header:
            # ç§»é™¤å¾®ç§’éƒ¨åˆ†ï¼Œé¿å…è§£æé—®é¢˜
            clean_time = reset_header.split('.')[0] + ('Z' if reset_header.endswith('Z') else '')
            if reset_header.endswith('Z'):
                reset_time = datetime.strptime(clean_time, "%Y-%m-%dT%H:%M:%SZ")
                reset_time = reset_time.replace(tzinfo=timezone.utc)
            else:
                reset_time = datetime.fromisoformat(clean_time)
            return int(reset_time.timestamp())
    except (ValueError, AttributeError):
        pass

    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å› Noneï¼Œä½¿ç”¨é»˜è®¤å€¼
    return None

def fetch_mastodon_posts(config, since_id=None, page_limit=None, max_posts=None):
    mastodon_config = config["mastodon"]
    instance_url, user_id, access_token = mastodon_config["instance_url"], mastodon_config["user_id"], mastodon_config["access_token"]
    api_url = f"{instance_url}/api/v1/accounts/{user_id}/statuses"
    headers = {"Authorization": f"Bearer {access_token}"}
    params = {"limit": POSTS_PER_REQUEST, "exclude_replies": False, "exclude_reblogs": True}
    if since_id: params["since_id"] = since_id
    all_posts, page_count = [], 1
    requests_in_window = 0
    window_start_time = time.time()

    while api_url:
        try:
            # æ™ºèƒ½é€Ÿç‡é™åˆ¶ç®¡ç†
            current_time = time.time()
            if current_time - window_start_time >= 300:  # 5 åˆ†é’Ÿçª—å£
                requests_in_window = 0
                window_start_time = current_time
                logging.info("ğŸ”„ é‡ç½® API è°ƒç”¨è®¡æ•°å™¨ï¼ˆ5 åˆ†é’Ÿçª—å£ï¼‰")

            # æ£€æŸ¥é€Ÿç‡é™åˆ¶
            if requests_in_window >= 280:  # ç•™ 20 æ¬¡ç¼“å†²
                wait_time = 300 - (current_time - window_start_time)
                if wait_time > 0:
                    logging.info(f"â±ï¸ æ¥è¿‘ API é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    # å®æ—¶å€’è®¡æ—¶æ˜¾ç¤º
                    total_wait = int(wait_time)
                    for remaining in range(total_wait, 0, -1):
                        progress = "â–ˆ" * ((total_wait - remaining) * 20 // total_wait)
                        empty = "â–‘" * (20 - len(progress))
                        # ä½¿ç”¨ \r å›åˆ°è¡Œé¦–ï¼Œå®ç°å€’è®¡æ—¶æ•ˆæœ
                        print(f"\râ³ ç­‰å¾…é‡ç½®ï¼š[{progress}{empty}] {remaining:3d}s ({(total_wait-remaining)*100//total_wait}%)  ", end="", flush=True)
                        time.sleep(1)
                    print()  # æ¢è¡Œ
                    logging.info("âœ… API é™åˆ¶å·²é‡ç½®ï¼Œç»§ç»­è·å–å¸–å­...")
                    requests_in_window = 0
                    window_start_time = time.time()

            # æ¯è·å– 100 é¡µæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æŠ¥å‘Š
            if page_count % 100 == 0 or page_count % 25 == 1:
                logging.info(f"ğŸ“Š è¿›åº¦æŠ¥å‘Šï¼šå·²è·å– {len(all_posts)} æ¡å¸–å­ï¼Œå…± {page_count} é¡µ")
            logging.info(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page_count} é¡µ...ï¼ˆå½“å‰çª—å£å·²è°ƒç”¨ {requests_in_window}/300 æ¬¡ï¼‰")
            response = requests.get(api_url, headers=headers, params=params)
            response.raise_for_status()

            # æ£€æŸ¥ API è¿”å›çš„é€Ÿç‡é™åˆ¶å¤´
            rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))

            # è§£æ X-RateLimit-Reset æ—¶é—´æˆ³
            reset_header = response.headers.get('X-RateLimit-Reset', '0')
            rate_limit_reset = parse_rate_limit_reset(reset_header)

            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå½“å‰æ—¶é—´ + é»˜è®¤ç­‰å¾…æ—¶é—´ï¼‰
            if rate_limit_reset is None:
                rate_limit_reset = int(time.time()) + DEFAULT_WAIT_TIME
                logging.warning(f"âš ï¸ æ— æ³•è§£æ API é‡ç½®æ—¶é—´æ ¼å¼ï¼š{reset_header}ï¼Œä½¿ç”¨é»˜è®¤ç­‰å¾…æ—¶é—´")

            posts = response.json()
            if not posts: break
            all_posts.extend(posts)
            requests_in_window += 1

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
            if page_limit and page_count >= page_limit: break
            if max_posts and len(all_posts) >= max_posts:
                all_posts = all_posts[:max_posts]
                break

            # å¦‚æœå‰©ä½™è°ƒç”¨æ¬¡æ•°å¾ˆå°‘ï¼Œç­‰å¾…é‡ç½®
            if rate_limit_remaining < RATE_LIMIT_THRESHOLD:
                reset_wait = max(0, rate_limit_reset - current_time)
                if reset_wait > 0 and reset_wait < 300:  # ä¸è¶…è¿‡ 5 åˆ†é’Ÿ
                    logging.info(f"â±ï¸ API è°ƒç”¨å³å°†ç”¨å®Œï¼Œç­‰å¾… {reset_wait:.1f} ç§’é‡ç½®...")
                    # å®æ—¶å€’è®¡æ—¶æ˜¾ç¤º
                    total_wait = int(reset_wait)
                    for remaining in range(total_wait, 0, -1):
                        progress = "â–ˆ" * ((total_wait - remaining) * 20 // total_wait)
                        empty = "â–‘" * (20 - len(progress))
                        print(f"\râ³ API é‡ç½®ï¼š[{progress}{empty}] {remaining:3d}s ({(total_wait-remaining)*100//total_wait}%)  ", end="", flush=True)
                        time.sleep(1)
                    print()  # æ¢è¡Œ
                    logging.info("âœ… API é™åˆ¶å·²é‡ç½®ï¼Œç»§ç»­è·å–å¸–å­...")
                    requests_in_window = 0

            links = requests.utils.parse_header_links(response.headers.get('Link', ''))
            api_url = next((link['url'] for link in links if link.get('rel') == 'next'), None)
            params.pop('since_id', None)
            page_count += 1

        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ API è¯·æ±‚å¤±è´¥ï¼š{e}")
            return []

    all_posts.reverse()
    logging.info(f"âœ… æˆåŠŸè·å– {len(all_posts)} æ¡å¸–å­ï¼Œå…±è°ƒç”¨ {page_count-1} æ¬¡ API")
    return all_posts

def download_media(media_item, media_folder_path):
    try:
        original_filename = Path(urlparse(media_item['url']).path).name
        local_filename = f"{media_item['id']}-{original_filename}"
        local_file_path = media_folder_path / local_filename
        if not local_file_path.exists():
            logging.info(f"â¬‡ï¸  æ­£åœ¨ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼š{media_item['url']}")
            response = requests.get(media_item['url'], stream=True)
            response.raise_for_status()
            with open(local_file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192): f.write(chunk)
        return local_filename
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ ä¸‹è½½åª’ä½“æ–‡ä»¶å¤±è´¥ï¼š{media_item['url']} - {e}")
        return None

def format_single_post_for_archive(post, media_folder_name, media_file_map, china_timezone=False):
    local_dt = get_timezone_aware_datetime(post["created_at"], china_timezone)
    time_str = local_dt.strftime("%H:%M")
    is_reply = post.get("in_reply_to_id")
    icon = "ğŸ’¬" if is_reply else "ğŸ“"
    heading = f"## {time_str} {icon} {'å›å¤' if is_reply else 'å˜Ÿæ–‡'}"
    source_link_text = "**å›å¤å˜Ÿæ–‡**" if is_reply else "**åŸå§‹å˜Ÿæ–‡**"
    content_md = md(post["content"], heading_style="ATX").strip()
    attachments_md = ""
    if post["media_attachments"]:
        media_parts = []
        for media in post["media_attachments"]:
            if (local_filename := media_file_map.get(media['id'])):
                media_path = f"{media_folder_name}/{local_filename}"
                media_parts.append(f"![{media.get('description') or 'Image'}]({media_path})")
        if media_parts:
            attachments_md = ("\n\n" if content_md else "") + "\n".join(media_parts)
    return f"{heading}\n\n**å†…å®¹**ï¼š{content_md}{attachments_md}\n\n{source_link_text}ï¼š{post['url']}\n\n---\n\n"

def format_post_for_single_file(post, media_folder_name, media_file_map, china_timezone=False):
    local_dt = get_timezone_aware_datetime(post["created_at"], china_timezone)
    frontmatter = {
        "id": post["id"], "createdAt": local_dt.strftime("%Y-%m-%d %H:%M:%S"),
        "source": post["url"], "type": "reply" if post["in_reply_to_id"] else "toot",
        "tags": [f"#{tag['name']}" for tag in post["tags"]]}
    if post["in_reply_to_id"]:
        frontmatter.update({"inReplyToId": post["in_reply_to_id"], "inReplyToAccountId": post["in_reply_to_account_id"]})
    yaml_frontmatter = "---\n" + yaml.dump(frontmatter, allow_unicode=True) + "---\n\n"
    content_md = md(post["content"], heading_style="ATX")
    attachments_md = ""
    if post["media_attachments"]:
        media_parts = []
        for media in post["media_attachments"]:
            if (local_filename := media_file_map.get(media['id'])):
                media_path = f"../{media_folder_name}/{local_filename}"
                media_parts.append(f"![{media.get('description') or 'Image'}]({media_path})\n")
        if media_parts: attachments_md = "\n## é™„ä»¶\n" + "".join(media_parts)
    return yaml_frontmatter + content_md + attachments_md

def update_archive_file(posts_to_update, config, all_posts_from_server, backup_path):
    backup_config = config["backup"]
    media_folder_name, archive_filename = backup_config["media_folder"], backup_config["filename"]
    archive_file_path = backup_path / archive_filename
    existing_posts_by_id = {}
    if archive_file_path.exists():
        with open(archive_file_path, 'r', encoding='utf-8') as f: content = f.read()
        for block in content.split('---\n'):
            if block.strip() and (match := re.search(r'https://[^\/]+\/[^\/]+\/(\d+)', block)):
                existing_posts_by_id[match.group(1)] = block
    for post in posts_to_update:
        existing_posts_by_id[post['id']] = format_single_post_for_archive(post, media_folder_name, config["media_file_map"], config["sync"]["china_timezone"]).strip()
    # ç¼–è¾‘æ£€æµ‹èŒƒå›´ï¼šæœ€è¿‘ 200 æ¡å¸–å­ï¼ˆçº¦ 1-2 å‘¨çš„å†…å®¹ï¼‰
    logging.info("ğŸ“ ç¼–è¾‘æ£€æµ‹èŒƒå›´ï¼šæœ€è¿‘ 200 æ¡å¸–å­ï¼Œè¦†ç›–çº¦ 1-2 å‘¨å†…çš„ç¼–è¾‘æ›´æ–°")
    rebuilt_posts_by_day = defaultdict(list)
    all_posts_dict = {p['id']: p for p in all_posts_from_server}
    for post_id, content in existing_posts_by_id.items():
        if (post_data := all_posts_dict.get(post_id)):
            local_dt = get_timezone_aware_datetime(post_data["created_at"], config["sync"]["china_timezone"])
            rebuilt_posts_by_day[local_dt.strftime("%Y-%m-%d")].append({'content': content, 'created_at': post_data['created_at']})
    final_content = ""
    for date_str in sorted(rebuilt_posts_by_day.keys(), reverse=True):
        final_content += f"# {date_str}\n\n"
        day_posts = sorted(rebuilt_posts_by_day[date_str], key=lambda p: p['created_at'], reverse=True)
        final_content += '\n'.join([p['content'] for p in day_posts]) + '\n\n'
    with open(archive_file_path, 'w', encoding='utf-8') as f: f.write(final_content)
    logging.info(f"âœï¸  å·²æ›´æ–°å½’æ¡£æ–‡ä»¶ï¼š{archive_file_path}")

def save_posts(posts, config, all_posts_from_server, backup_path):
    backup_config = config["backup"]
    posts_folder_path = backup_path / backup_config["posts_folder"]
    media_folder_path = backup_path / backup_config["media_folder"]
    media_file_map = {}
    for post in posts:
        if post.get("media_attachments"):
            media_folder_path.mkdir(parents=True, exist_ok=True)
            for media in post["media_attachments"]:
                if (local_filename := download_media(media, media_folder_path)):
                    media_file_map[media['id']] = local_filename
    config["media_file_map"] = media_file_map
    update_archive_file(posts, config, all_posts_from_server, backup_path)
    posts_folder_path.mkdir(parents=True, exist_ok=True)
    for post in posts:
        local_dt = get_timezone_aware_datetime(post["created_at"], config["sync"]["china_timezone"])
        filename = f"{local_dt.strftime('%Y-%m-%d_%H%M%S')}_{post['id']}.md"
        file_path = posts_folder_path / filename
        new_content = format_post_for_single_file(post, backup_config["media_folder"], media_file_map, config["sync"]["china_timezone"])
        if not file_path.exists() or file_path.read_text('utf-8') != new_content:
            try:
                file_path.write_text(new_content, encoding='utf-8')
                logging.info(f"ğŸ“„ å·²å¤‡ä»½/æ›´æ–°å•æ¡å˜Ÿæ–‡ï¼š{file_path}")
            except IOError as e:
                logging.error(f"âŒ æ— æ³•å†™å…¥æ–‡ä»¶ {file_path}: {e}")

def get_color_from_count(count):
    if count == 0: return "#ebedf0"
    if 1 <= count <= 2: return "#9be9a8"
    if 3 <= count <= 5: return "#40c463"
    if 6 <= count <= 9: return "#30a14e"
    return "#216e39"

def generate_heatmap_svg(post_counts, year, output_path, username="", instance=""):
    logging.info(f"ğŸ¨ æ­£åœ¨ä¸º {year} å¹´ç”Ÿæˆ SVG çƒ­åŠ›å›¾...")
    SQUARE_SIZE, SPACING = 10, 3; SQUARE_TOTAL_SIZE = SQUARE_SIZE + SPACING
    X_OFFSET, Y_OFFSET = 25, 35; WIDTH = X_OFFSET + SQUARE_TOTAL_SIZE * 53 + SPACING; HEIGHT = Y_OFFSET + SQUARE_TOTAL_SIZE * 7 + 20

    # è®¡ç®—å˜Ÿæ–‡æ€»æ•°
    total_posts = sum(post_counts.values())

    svg_parts = [f'<svg width="{WIDTH}" height="{HEIGHT}" xmlns="http://www.w3.org/2000/svg">', '<style>.month-label, .wday-label, .year-label, .total-label, .user-label { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji"; font-size: 9px; fill: #767676; } .year-label { font-size: 14px; font-weight: 600; } .total-label { font-size: 11px; font-weight: 500; } .user-label { font-size: 10px; }</style>']
    # æ·»åŠ å¹´ä»½æ ‡ç­¾ï¼ˆå·¦ä¸Šè§’ï¼‰
    svg_parts.append(f'<text x="0" y="15" class="year-label">{year}</text>')
    # æ·»åŠ å˜Ÿæ–‡æ€»æ•°ï¼ˆå³ä¸Šè§’ï¼‰
    svg_parts.append(f'<text x="{WIDTH - 5}" y="15" class="total-label" text-anchor="end">å…± {total_posts} æ¡å˜Ÿæ–‡</text>')
    # æ·»åŠ ç”¨æˆ·ä¿¡æ¯ï¼ˆå³ä¸‹è§’ï¼‰
    if username and instance:
        user_text = f"@{username}@{instance}"
        svg_parts.append(f'<text x="{WIDTH - 5}" y="{HEIGHT - 5}" class="user-label" text-anchor="end">{user_text}</text>')
    for day, label in {1: "M", 3: "W", 5: "F"}.items():
        svg_parts.append(f'<text x="0" y="{Y_OFFSET + day * SQUARE_TOTAL_SIZE + SQUARE_SIZE}" class="wday-label">{label}</text>')
    year_start, month_labels = date(year, 1, 1), {}
    year_start_weekday = (year_start.weekday() + 1) % 7
    days_in_year = 366 if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0) else 365
    for day_of_year in range(days_in_year):
        current_date = year_start + timedelta(days=day_of_year)
        if current_date.year != year: continue
        if current_date.day == 1:  # ä¿®å¤ï¼šç§»é™¤ day_of_year > 0 æ¡ä»¶ï¼Œè®© 1 æœˆä¹Ÿèƒ½æ˜¾ç¤º
            month_labels[(day_of_year + year_start_weekday) // 7] = current_date.strftime("%b")
        count = post_counts.get(current_date, 0)
        total_days = day_of_year + year_start_weekday
        x_pos, y_pos = X_OFFSET + (total_days // 7) * SQUARE_TOTAL_SIZE, Y_OFFSET + (total_days % 7) * SQUARE_TOTAL_SIZE
        tooltip = f"{current_date.strftime('%Y-%m-%d')}: {count} post{'s' if count != 1 else ''}"
        svg_parts.append(f'<rect x="{x_pos}" y="{y_pos}" width="{SQUARE_SIZE}" height="{SQUARE_SIZE}" fill="{get_color_from_count(count)}" rx="2" ry="2"><title>{tooltip}</title></rect>')
    for week, month in month_labels.items():
        svg_parts.append(f'<text x="{X_OFFSET + week * SQUARE_TOTAL_SIZE}" y="{Y_OFFSET - 8}" class="month-label">{month}</text>')
    svg_parts.append("</svg>")
    with open(output_path, 'w', encoding='utf-8') as f: f.write("\n".join(svg_parts))
    logging.info(f"âœ… çƒ­åŠ›å›¾å·²æˆåŠŸç”Ÿæˆè‡³ '{output_path.name}'ã€‚")

def generate_activity_summary(config, backup_path):
    logging.info("ğŸ“Š æ­£åœ¨ç”Ÿæˆæ´»åŠ¨æ€»ç»“æŠ¥å‘Š...")
    backup_config = config["backup"]
    posts_folder_path = backup_path / backup_config["posts_folder"]
    summary_filepath = backup_path / backup_config["summary_filename"]

    if not posts_folder_path.exists() or not any(posts_folder_path.iterdir()):
        logging.warning("âš ï¸ æœªæ‰¾åˆ°å¸–å­å¤‡ä»½æ–‡ä»¶å¤¹æˆ–æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“æŠ¥å‘Šã€‚")
        return

    all_posts = []
    for post_file in sorted(list(posts_folder_path.glob("*.md")), reverse=True):
        try:
            with open(post_file, 'r', encoding='utf-8') as f: content = f.read()
            parts = content.split('---', 2)
            if len(parts) < 3: continue
            frontmatter = yaml.safe_load(parts[1])
            all_posts.append({
                "datetime": datetime.strptime(frontmatter["createdAt"], "%Y-%m-%d %H:%M:%S"),
                "content": parts[2].strip().replace('../media/', './media/'),
                "source": frontmatter.get("source", ""), "type": frontmatter.get("type", "toot")
            })
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ–‡ä»¶ {post_file.name} æ—¶å‡ºé”™ï¼š{e}")

    if not all_posts:
        summary_filepath.write_text("# Mastodon æ´»åŠ¨æ€»ç»“\n\næœªæ‰¾åˆ°ä»»ä½•å¸–å­æ¥ç”ŸæˆæŠ¥å‘Šã€‚", encoding='utf-8')
        return

    # è·å–ç”¨æˆ·ä¿¡æ¯
    username = config.get("username", "")
    instance = config.get("instance", "")

    # å¦‚æœ config ä¸­æ²¡æœ‰ç”¨æˆ·ä¿¡æ¯ï¼Œå°è¯•ä»ç¬¬ä¸€ä¸ªå¸–å­çš„ frontmatter source URL ä¸­æå–
    if (not username or not instance) and posts_folder_path.exists():
        try:
            first_post_file = sorted(list(posts_folder_path.glob("*.md")))[0]
            with open(first_post_file, 'r', encoding='utf-8') as f:
                content = f.read()
            parts = content.split('---', 2)
            if len(parts) >= 3:
                frontmatter = yaml.safe_load(parts[1])
                source_url = frontmatter.get("source", "")
                # ä» source URL æå–ï¼šhttps://instance/@username/123456
                if source_url and "@" in source_url:
                    url_parts = source_url.split("//")[1] if "//" in source_url else source_url
                    instance = url_parts.split("/")[0]
                    username_part = url_parts.split("@")[1].split("/")[0] if "@" in url_parts else ""
                    username = username_part if username_part else username
        except Exception as e:
            logging.warning(f"æ— æ³•ä»å¸–å­ä¸­æå–ç”¨æˆ·ä¿¡æ¯ï¼š{e}")

    # æŒ‰å¹´ä»½åˆ†ç»„å¸–å­
    posts_by_year = defaultdict(lambda: defaultdict(int))
    for post in all_posts:
        year = post["datetime"].year
        post_date = post["datetime"].date()
        posts_by_year[year][post_date] += 1

    # è·å–æ‰€æœ‰å¹´ä»½å¹¶æ’åºï¼ˆä»æ–°åˆ°æ—§ï¼‰
    all_years = sorted(posts_by_year.keys(), reverse=True)

    today = date.today()
    final_md = f"# Mastodon æ´»åŠ¨æ€»ç»“\n\næˆªè‡³ {today.strftime('%Y-%m-%d')} çš„æ´»åŠ¨æ¦‚è§ˆã€‚\n\n"

    # ä¸ºæ¯ä¸ªå¹´ä»½ç”Ÿæˆçƒ­åŠ›å›¾
    for year in all_years:
        heatmap_svg_filename = f"heatmap-{year}.svg"
        heatmap_svg_filepath = backup_path / heatmap_svg_filename

        # ç”Ÿæˆè¯¥å¹´ä»½çš„çƒ­åŠ›å›¾
        generate_heatmap_svg(posts_by_year[year], year, heatmap_svg_filepath, username, instance)

        # è®¡ç®—è¯¥å¹´ä»½çš„æ€»å˜Ÿæ–‡æ•°
        total_posts_this_year = sum(posts_by_year[year].values())

        # æ·»åŠ åˆ° Markdown
        final_md += (f"## {year} å¹´æ´»åŠ¨çƒ­åŠ›å›¾\n\n"
                    f"è¯¥å¹´å…±å‘å¸ƒäº† {total_posts_this_year} ç¯‡å˜Ÿæ–‡ã€‚\n\n"
                    f"![{year} Activity Heatmap](./{heatmap_svg_filename})\n\n")

    summary_filepath.write_text(final_md, encoding='utf-8')
    logging.info(f"âœ… æ´»åŠ¨æ€»ç»“æŠ¥å‘Šå·²æˆåŠŸæ›´æ–°è‡³ '{summary_filepath.name}'ã€‚")
    logging.info(f"âœ… å…±ç”Ÿæˆ {len(all_years)} ä¸ªå¹´ä»½çš„çƒ­åŠ›å›¾ã€‚")

def generate_mastodon_html(posts, config, backup_path):
    """ç”Ÿæˆå•æ–‡ä»¶ HTML ç½‘é¡µï¼Œå¤åˆ» Mastodon ç•Œé¢"""
    backup_config = config["backup"]
    html_filename = backup_config.get("html_filename", "index.html")
    html_filepath = backup_path / html_filename
    media_folder = backup_config["media_folder"]

    logging.info("æ­£åœ¨ç”Ÿæˆ Mastodon HTML ç½‘é¡µ...")

    # æå–ç”¨æˆ·ä¿¡æ¯
    if posts:
        user = posts[0]["account"]
        username = user["username"]
        display_name = user.get("display_name", username)
        avatar = user["avatar"]
        user_id = user["id"]
        # ä» URL ä¸­æå–å®ä¾‹åç§°
        user_url = user["url"]
        instance_name = user_url.split("//")[1].split("/")[0] if "//" in user_url else ""
        # è·å–ç”¨æˆ·ç®€ä»‹
        user_bio = user.get("note", "")  # note å­—æ®µåŒ…å«ç”¨æˆ·çš„ç®€ä»‹
        # ä¿å­˜ç”¨æˆ·ä¿¡æ¯åˆ°é…ç½®ä¸­ï¼Œä¾›çƒ­åŠ›å›¾ä½¿ç”¨
        config["username"] = username
        config["instance"] = instance_name
        # è·å–ç”¨æˆ·èƒŒæ™¯å›¾ç‰‡
        header = user.get("header", "")
        if header:
            header_filename = f"header-{user_id}.jpg"
            local_header_path = f"{media_folder}/{header_filename}"

            # ä¸‹è½½èƒŒæ™¯å›¾ç‰‡
            try:
                header_response = requests.get(header, stream=True)
                if header_response.status_code == 200:
                    header_path = backup_path / media_folder / header_filename
                    header_path.parent.mkdir(exist_ok=True)
                    with open(header_path, 'wb') as f:
                        for chunk in header_response.iter_content(1024):
                            f.write(chunk)
                    background_image = local_header_path
                else:
                    background_image = ""
            except Exception as e:
                logging.warning(f"ä¸‹è½½èƒŒæ™¯å›¾ç‰‡å¤±è´¥ï¼š{e}")
                background_image = ""
        else:
            background_image = ""
    else:
        username = "unknown"
        display_name = "Unknown User"
        avatar = ""
        user_id = "unknown"
        instance_name = ""
        background_image = ""

    # æå–ç»Ÿè®¡æ•°æ®
    total_posts = len(posts)
    followers_count = posts[0]["account"]["followers_count"] if posts else 0
    following_count = posts[0]["account"]["following_count"] if posts else 0

    # è½¬æ¢å¸–å­æ•°æ®ä¸º JSON
    posts_data = []
    for post in posts:
        # éªŒè¯å¸–å­æ•°æ®å®‰å…¨æ€§
        if not validate_post_data(post):
            logging.warning(f"è·³è¿‡æ— æ•ˆçš„å¸–å­æ•°æ®ï¼ŒID: {post.get('id', 'unknown')}")
            continue
        # å¤„ç†åª’ä½“é™„ä»¶
        media_items = []
        for media in post.get("media_attachments", []):
            media_filename = f"{media['id']}-{Path(urlparse(media['url']).path).name}"
            media_items.append({
                "id": media["id"],
                "type": media["type"],
                "url": f"{media_folder}/{media_filename}",
                "description": media.get("description", ""),
                "preview_url": media.get("preview_url", "")
            })

        # å¤„ç†å†…å®¹ HTML å’Œè¡¨æƒ…ç¬¦å·
        content_html = post.get("content", "")

        # å¤„ç† Mastodon è¡¨æƒ…ç¬¦å· - è½¬æ¢ä¸º base64 åµŒå…¥
        emojis = post.get("emojis", [])
        for emoji in emojis:
            shortcode = emoji.get("shortcode", "")
            url = emoji.get("url", "")
            static_url = emoji.get("static_url", url)
            if shortcode and static_url:
                # ä¸‹è½½ emoji å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
                try:
                    emoji_response = requests.get(static_url, timeout=10)
                    if emoji_response.status_code == 200:
                        emoji_base64 = base64.b64encode(emoji_response.content).decode('utf-8')
                        # æ£€æµ‹å›¾ç‰‡ç±»å‹
                        content_type = emoji_response.headers.get('Content-Type', 'image/png')
                        # ç”Ÿæˆ data URI
                        data_uri = f"data:{content_type};base64,{emoji_base64}"
                        # æ›¿æ¢ emoji
                        emoji_pattern = f":{shortcode}:"
                        emoji_img_tag = f'<img src="{data_uri}" alt=":{shortcode}:" class="custom-emoji" title=":{shortcode}:" loading="lazy">'
                        content_html = content_html.replace(emoji_pattern, emoji_img_tag)
                except Exception as e:
                    logging.warning(f"âš ï¸ ä¸‹è½½ emoji å¤±è´¥ {shortcode}: {e}")
                    # å¤±è´¥æ—¶ä½¿ç”¨è¿œç¨‹ URL
                    emoji_pattern = f":{shortcode}:"
                    emoji_img_tag = f'<img src="{static_url}" alt=":{shortcode}:" class="custom-emoji" title=":{shortcode}:" loading="lazy">'
                    content_html = content_html.replace(emoji_pattern, emoji_img_tag)

        # å¤„ç†æ—¶é—´
        created_at = post["created_at"]
        local_time = get_timezone_aware_datetime(created_at, config["sync"]["china_timezone"])

        post_data = {
            "id": post["id"],
            "content": content_html,
            "created_at": local_time.strftime("%Y-%m-%d %H:%M:%S"),
            "timestamp": created_at,  # ä½¿ç”¨åŸå§‹ ISO æ ¼å¼æ—¶é—´å­—ç¬¦ä¸²
            "url": post["url"],
            "sensitive": post.get("sensitive", False),
            "spoiler_text": post.get("spoiler_text", ""),
            "visibility": post.get("visibility", "public"),
            "media_attachments": media_items,
            "reblogs_count": post.get("reblogs_count", 0),
            "favourites_count": post.get("favourites_count", 0),
            "replies_count": post.get("replies_count", 0),
            "in_reply_to_id": post.get("in_reply_to_id", None),
            "in_reply_to_account_id": post.get("in_reply_to_account_id", None),
            "account": {
                "id": user_id,
                "username": post["account"]["username"],
                "display_name": post["account"].get("display_name", post["account"]["username"]),
                "url": post["account"]["url"],
                "avatar": post["account"]["avatar"]
            },
            "tags": [{"name": tag["name"]} for tag in post.get("tags", [])]
        }
        posts_data.append(post_data)

    # ç”Ÿæˆ HTML å†…å®¹
    html_content = generate_html_template(username, display_name, avatar, instance_name, background_image,
                                        total_posts, followers_count, following_count, posts_data, user_bio)

    # å†™å…¥ HTML æ–‡ä»¶
    with open(html_filepath, 'w', encoding='utf-8') as f:
        f.write(html_content)

    logging.info(f"HTML ç½‘é¡µå·²ç”Ÿæˆè‡³ï¼š{html_filepath}")
    logging.info(f"åŒ…å« {total_posts} æ¡å˜Ÿæ–‡")
    logging.info(f"å›¾ç‰‡è·¯å¾„ï¼š{media_folder}/")

def generate_html_template(username, display_name, avatar, instance_name, background_image,
                          total_posts, followers_count, following_count,
                          posts_data, user_bio):
    """ç”Ÿæˆå®Œæ•´çš„ HTML é¡µé¢"""

    # å°† posts_data è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
    posts_json = json.dumps(posts_data, ensure_ascii=False)

    # ä½¿ç”¨ template.py ç”Ÿæˆ HTML
    return template.generate_html(
        username=username,
        display_name=display_name,
        avatar=avatar,
        instance_name=instance_name,
        background_image=background_image,
        total_posts=total_posts,
        followers_count=followers_count,
        following_count=following_count,
        posts_json=posts_json,
        user_bio=user_bio
    )


def safe_remove_directory(path):
    """
    å®‰å…¨åˆ é™¤ç›®å½•ï¼Œå¤„ç†æƒé™é—®é¢˜å’Œæ–‡ä»¶é”å®š
    """
    if not path.exists():
        return True
    
    try:
        # é¦–å…ˆå°è¯•æ­£å¸¸åˆ é™¤
        shutil.rmtree(path)
        return True
    except PermissionError as e:
        logging.warning(f"âš ï¸ æƒé™é”™è¯¯ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤ç›®å½• {path}: {e}")
        
        # å°è¯•ç§»é™¤åªè¯»å±æ€§
        try:
            for root, dirs, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        os.chmod(file_path, 0o777)
                    except (OSError, PermissionError):
                        pass
                for dir in dirs:
                    dir_path = os.path.join(root, dir)
                    try:
                        os.chmod(dir_path, 0o777)
                    except (OSError, PermissionError):
                        pass
        except Exception as e:
            logging.warning(f"âš ï¸ ç§»é™¤åªè¯»å±æ€§å¤±è´¥ï¼š{e}")
        
        # å†æ¬¡å°è¯•åˆ é™¤
        try:
            shutil.rmtree(path)
            return True
        except PermissionError:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•é€ä¸ªåˆ é™¤æ–‡ä»¶
            try:
                import stat
                def remove_readonly(func, path, excinfo):
                    os.chmod(path, stat.S_IWRITE)
                    func(path)
                
                shutil.rmtree(path, onerror=remove_readonly)
                return True
            except Exception as e:
                logging.error(f"âŒ æ— æ³•åˆ é™¤ç›®å½• {path}: {e}")
                return False
    except Exception as e:
        logging.error(f"âŒ åˆ é™¤ç›®å½• {path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
        return False

def safe_remove_file(path):
    """
    å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†æƒé™é—®é¢˜
    """
    if not path.exists():
        return True
    
    try:
        path.unlink()
        return True
    except PermissionError as e:
        logging.warning(f"âš ï¸ æƒé™é”™è¯¯ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤æ–‡ä»¶ {path}: {e}")
        try:
            # å°è¯•ç§»é™¤åªè¯»å±æ€§
            os.chmod(path, 0o777)
            path.unlink()
            return True
        except Exception as e:
            logging.error(f"âŒ æ— æ³•åˆ é™¤æ–‡ä»¶ {path}: {e}")
            return False
    except Exception as e:
        logging.error(f"âŒ åˆ é™¤æ–‡ä»¶ {path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
        return False

def should_update_summary(is_full_sync, new_posts_count, backup_path, backup_config):
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ´»åŠ¨æ€»ç»“
    """
    summary_filepath = backup_path / backup_config["summary_filename"]
    
    # å…¨é‡åŒæ­¥æ—¶æ€»æ˜¯æ›´æ–°
    if is_full_sync:
        return True
    
    # é¦–æ¬¡è¿è¡Œæ—¶æ›´æ–°
    if not summary_filepath.exists():
        return True
    
    # æœ‰æ–°å¸–å­æ—¶æ›´æ–°
    if new_posts_count > 0:
        return True
    
    return False

def main():
    logging.info("========================================")
    logging.info(" Mastodon Sync å¼€å§‹è¿è¡Œ")
    logging.info("========================================")
    config = get_config()

    # éªŒè¯é…ç½®
    try:
        validate_config(config)
    except ValueError as e:
        logging.error(f"é…ç½®éªŒè¯å¤±è´¥ï¼š{e}")
        return

    # æ ¹æ®é…ç½®ç¡®å®šæœ€ç»ˆçš„å¤‡ä»½è·¯å¾„
    base_path_str = config["backup"]["path"]
    backup_path = Path(base_path_str)
    # ä»…åœ¨æœ¬åœ°è¿è¡Œæ—¶ï¼Œå¦‚æœè·¯å¾„ä¸æ˜¯é»˜è®¤çš„"."ï¼Œæ‰æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if not os.environ.get("GITHUB_ACTIONS") and base_path_str != ".":
         logging.info(f"ğŸ’¾ æ‰€æœ‰å¤‡ä»½æ–‡ä»¶å°†ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼š{backup_path.resolve()}")
    backup_path.mkdir(parents=True, exist_ok=True)
    
    # çŠ¶æ€æ–‡ä»¶æ€»æ˜¯å’Œè„šæœ¬æ”¾åœ¨ä¸€èµ·ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œä¸éšå¤‡ä»½è·¯å¾„æ”¹å˜
    state_file_path = Path(config["sync"]["state_file"])

    backup_config = config["backup"]
    archive_file_path = backup_path / backup_config["filename"]
    posts_folder_path = backup_path / backup_config["posts_folder"]
    media_folder_path = backup_path / backup_config["media_folder"]

    is_cli_full_sync = "--full-sync" in sys.argv
    is_action_full_sync = os.environ.get("FORCE_FULL_SYNC") == "true"
    is_first_run = not archive_file_path.exists()
    is_manual_full_sync = is_cli_full_sync or is_action_full_sync
    is_full_sync = is_manual_full_sync or is_first_run
    config["is_full_sync"] = is_full_sync

    if is_full_sync:
        if is_first_run:
            logging.info("ğŸ†• æ£€æµ‹åˆ°é¦–æ¬¡è¿è¡Œï¼Œå°†å¼€å§‹åˆå§‹åŒ–å¤‡ä»½...")
        else:
            logging.warning("âš ï¸  æ£€æµ‹åˆ°å…¨é‡åŒæ­¥æ¨¡å¼ï¼Œå°†æ¸…ç†ç›®æ ‡è·¯å¾„ä¸‹çš„æ—§å¤‡ä»½æ–‡ä»¶...")
        
        # çŠ¶æ€æ–‡ä»¶åœ¨é¡¹ç›®ç›®å½•ï¼Œä¹Ÿè¦æ¸…ç†
        if not safe_remove_file(state_file_path):
            logging.error("âŒ æ— æ³•åˆ é™¤çŠ¶æ€æ–‡ä»¶ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        
        if not safe_remove_file(archive_file_path):
            logging.error("âŒ æ— æ³•åˆ é™¤å½’æ¡£æ–‡ä»¶ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        
        if not safe_remove_directory(posts_folder_path):
            logging.error("âŒ æ— æ³•åˆ é™¤å¸–å­æ–‡ä»¶å¤¹ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æ˜¯å¦æœ‰ç¨‹åºå ç”¨è¯¥æ–‡ä»¶å¤¹")
            logging.error("âŒ å»ºè®®ï¼šå…³é—­å¯èƒ½å ç”¨æ–‡ä»¶å¤¹çš„ç¨‹åºï¼ˆå¦‚æ–‡ä»¶æµè§ˆå™¨ã€OneDrive åŒæ­¥ç­‰ï¼‰åé‡è¯•")
            # é€‰æ‹©æ€§é€€å‡ºï¼Œå› ä¸ºæ— æ³•åˆ é™¤æ–‡ä»¶å¤¹å¯èƒ½ä¼šå¯¼è‡´åç»­æ“ä½œå¤±è´¥
            if posts_folder_path.exists():
                logging.error("âŒ ç”±äºæ— æ³•æ¸…ç†æ—§æ–‡ä»¶ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œç¨‹åºå°†é€€å‡º")
                sys.exit(1)
        
        if not safe_remove_directory(media_folder_path):
            logging.error("âŒ æ— æ³•åˆ é™¤åª’ä½“æ–‡ä»¶å¤¹ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
    
    last_synced_id = None
    if not is_full_sync and state_file_path.exists():
        try:
            last_synced_id = json.loads(state_file_path.read_text())['last_synced_id']
        except (json.JSONDecodeError, KeyError):
            logging.warning("âš ï¸ åŒæ­¥çŠ¶æ€æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†æ‰§è¡Œå…¨é‡åŒæ­¥ã€‚")
            is_full_sync = True
            config["is_full_sync"] = True

    if is_full_sync:
        # æ™ºèƒ½å…¨é‡åŒæ­¥ï¼šå……åˆ†åˆ©ç”¨ API é™åˆ¶è·å–æ‰€æœ‰å¸–å­
        # æ— é¡µæ•°é™åˆ¶ï¼Œé€šè¿‡æ™ºèƒ½é€Ÿç‡ç®¡ç†å®‰å…¨è·å–æ‰€æœ‰å†å²å¸–å­
        logging.info("ğŸ”„ æ™ºèƒ½å…¨é‡åŒæ­¥æ¨¡å¼ï¼Œå°†è·å–æ‰€æœ‰å†å²å¸–å­...")
        logging.info("âš¡ ç³»ç»Ÿå°†æ™ºèƒ½ç®¡ç† API é€Ÿç‡é™åˆ¶ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´å®Œæˆ")
        posts_to_process = fetch_mastodon_posts(config)
        all_posts_from_server_for_sync = posts_to_process
        new_posts_count = len(posts_to_process)
        logging.info(f"ğŸ“Š å…¨é‡åŒæ­¥å®Œæˆï¼Œå…±è·å– {new_posts_count} æ¡å†å²å¸–å­")
    else:
        new_posts = fetch_mastodon_posts(config, since_id=last_synced_id)
        # è·å–æ›´å¤šå¸–å­ç”¨äºç¼–è¾‘æ£€æµ‹ï¼ˆ5 é¡µï¼Œ200 æ¡å¸–å­ï¼‰ï¼Œè¦†ç›–è¿‡å» 1-2 å‘¨çš„ç¼–è¾‘
        recent_posts = fetch_mastodon_posts(config, page_limit=5)
        all_posts_from_server_for_sync = recent_posts
        posts_dict = {p['id']: p for p in new_posts}
        for p in recent_posts: posts_dict[p['id']] = p
        posts_to_process = sorted(list(posts_dict.values()), key=lambda p: p['created_at'])
        new_posts_count = len(new_posts)

    if posts_to_process:
        save_posts(posts_to_process, config, all_posts_from_server_for_sync, backup_path)
        all_ids = [p['id'] for p in posts_to_process]
        if last_synced_id and not is_full_sync: all_ids.append(last_synced_id)
        if all_ids: 
            state_file_path.write_text(json.dumps({"last_synced_id": max(all_ids, key=int)}))
    else:
        logging.info("âœ¨ æ²¡æœ‰æ–°å†…å®¹éœ€è¦åŒæ­¥ã€‚")
        
    # ç»Ÿä¸€çš„æ´»åŠ¨æ€»ç»“æ›´æ–°é€»è¾‘ï¼šæ™ºèƒ½åŒæ­¥å’Œå…¨é‡åŒæ­¥éƒ½ä¼šæ›´æ–°ç»Ÿè®¡
    if should_update_summary(is_full_sync, new_posts_count, backup_path, backup_config):
        if is_full_sync:
            logging.info("ğŸ”„ å…¨é‡åŒæ­¥æ¨¡å¼ï¼Œç”Ÿæˆæ´»åŠ¨æ€»ç»“...")
        else:
            logging.info("ğŸ“Š æ£€æµ‹åˆ°æ–°å†…å®¹ï¼Œæ›´æ–°æ´»åŠ¨æ€»ç»“...")
        generate_activity_summary(config, backup_path)
    else:
        logging.info("ğŸ“Š æ²¡æœ‰æ–°å†…å®¹éœ€è¦æ›´æ–°ï¼Œè·³è¿‡æ´»åŠ¨æ€»ç»“ç”Ÿæˆã€‚")

    # ç”Ÿæˆ HTML ç½‘é¡µ - æ™ºèƒ½æ£€æµ‹æ˜¯å¦éœ€è¦æ›´æ–°
    try:
        html_filename = backup_config.get("html_filename", "index.html")
        html_filepath = backup_path / html_filename
        should_generate_html = False
        posts_for_html = None

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆ HTML
        if not html_filepath.exists():
            logging.info("ğŸŒ HTML æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡†å¤‡é¦–æ¬¡ç”Ÿæˆ...")
            should_generate_html = True
        elif is_full_sync:
            logging.info("ğŸ”„ å…¨é‡åŒæ­¥æ¨¡å¼ï¼Œå°†é‡æ–°ç”Ÿæˆ HTML...")
            should_generate_html = True
        elif new_posts_count > 0:
            logging.info(f"ğŸ“Š æ£€æµ‹åˆ° {new_posts_count} æ¡æ–°å¸–å­ï¼Œéœ€è¦æ›´æ–° HTML...")
            should_generate_html = True
        else:
            logging.info("âœ… HTML æ–‡ä»¶å·²å­˜åœ¨ä¸”æ— æ–°å†…å®¹ï¼Œè·³è¿‡ç”Ÿæˆ")

        # å¦‚æœéœ€è¦ç”Ÿæˆ HTMLï¼Œè·å–æ‰€æœ‰å¸–å­æ•°æ®
        if should_generate_html:
            if is_full_sync and posts_to_process:
                # å…¨é‡åŒæ­¥æ—¶ï¼Œposts_to_process å·²åŒ…å«æ‰€æœ‰å¸–å­
                logging.info(f"ğŸ“Š ä½¿ç”¨å…¨é‡åŒæ­¥æ•°æ® ({len(posts_to_process)} æ¡å¸–å­)")
                posts_for_html = posts_to_process
            else:
                # å¢é‡åŒæ­¥æˆ–å…¨é‡åŒæ­¥å¤±è´¥æ—¶ï¼Œé‡æ–°è·å–æ‰€æœ‰å¸–å­
                logging.info("ğŸ“Š æ­£åœ¨è·å–æ‰€æœ‰å¸–å­ç”¨äº HTML ç”Ÿæˆ...")
                all_posts_for_html = fetch_mastodon_posts(config)
                if all_posts_for_html:
                    logging.info(f"ğŸ“Š æˆåŠŸè·å– {len(all_posts_for_html)} æ¡å¸–å­")
                    posts_for_html = all_posts_for_html
                else:
                    logging.error("âŒ æ— æ³•ä» API è·å–å¸–å­æ•°æ®")

            # ç”Ÿæˆ HTML
            if posts_for_html:
                generate_mastodon_html(posts_for_html, config, backup_path)
                logging.info(f"âœ… HTML ç½‘é¡µå·²ç”Ÿæˆï¼ŒåŒ…å« {len(posts_for_html)} æ¡å˜Ÿæ–‡")
            else:
                logging.error("âŒ æ²¡æœ‰å¯ç”¨çš„å¸–å­æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆ HTML")

    except Exception as e:
        logging.error(f"âŒ HTML ç½‘é¡µç”Ÿæˆå¤±è´¥ï¼š{e}")

    logging.info("========================================")
    logging.info("åŒæ­¥å®Œæˆï¼")
    logging.info("========================================")

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    main()
